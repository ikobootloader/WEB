<!DOCTYPE html>
<html lang="fr">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Exploration optimale avec récompenses et obstacles</title>
    <style>
        canvas {
            border: 1px solid black;
            background-color: #f0f0f0;
        }
        .info {
            margin-top: 10px;
        }
        .explanation {
            margin-left: 20px;
            max-width: 400px;
			max-height: 600px;
			overflow: auto;
            font-size: 14px;
            line-height: 1.5;
        }
        .container {
            display: flex;
            align-items: flex-start;
        }
    </style>
</head>
<body>
    <div class="container">
        <canvas id="territory" width="600" height="600"></canvas>
        <div class="explanation">
            <h2>Explication détaillée du code</h2>
            <p>
                Ce code JavaScript implémente une simulation où un agent explore une grille représentant un territoire. La grille est divisée en cellules, et chaque cellule peut contenir un obstacle, une récompense, ou être vide.
                L'agent doit naviguer dans cet environnement pour collecter des récompenses tout en évitant les obstacles.
            </p>
            <h3>Initialisation de l'environnement</h3>
            <p>
                Le script commence par initialiser le canvas où la simulation sera affichée, ainsi que diverses variables comme la taille des cellules, la santé de l'agent, et les paramètres de la grille. Les obstacles et les récompenses sont générés aléatoirement sur la grille.
            </p>
            <h3>Fonctions principales</h3>
            <h4>1. <code>generateObstaclesAndRewards()</code></h4>
            <p>
                Cette fonction place aléatoirement des obstacles et des récompenses sur la grille. Les obstacles sont dessinés en rouge et les récompenses en vert. Chaque récompense a une valeur qui sera utilisée pour calculer l'attractivité des positions voisines.
            </p>
            <h4>2. <code>drawObstacle(x, y)</code> et <code>drawReward(x, y)</code></h4>
            <p>
                Ces fonctions dessinent respectivement un obstacle ou une récompense à la position (x, y) sur la grille.
            </p>
            <h4>3. <code>drawAgent(x, y)</code> et <code>clearAgent(x, y)</code></h4>
            <p>
                <code>drawAgent()</code> place l'agent à la position (x, y) sur la grille, représenté par un carré bleu. <code>clearAgent()</code> efface l'agent de la position précédente avant qu'il ne se déplace.
            </p>
            <h4>4. <code>discoverSurroundings()</code></h4>
            <p>
                Cette fonction permet à l'agent de découvrir les cellules autour de lui. Si une récompense ou un obstacle est détecté à proximité immédiate, il est ajouté aux listes des obstacles et récompenses découverts, et est dessiné sur la grille.
            </p>
            <h4>5. <code>propagateValues()</code></h4>
            <p>
                Après la découverte des obstacles et récompenses, cette fonction calcule les valeurs d'état pour chaque cellule de la grille. Les cellules proches des récompenses reçoivent une valeur plus élevée, indiquant qu'elles sont plus attractives pour l'agent.
                Les cellules contenant des obstacles reçoivent une valeur négative pour indiquer qu'elles doivent être évitées.
            </p>
            <h4>6. <code>findBestDirection()</code></h4>
            <p>
                Cette fonction détermine la meilleure direction pour l'agent en fonction des valeurs d'état calculées et du mode actuel (exploration ou survie). En mode survie, l'agent se dirige vers les cellules avec des valeurs d'état élevées, indiquant la présence possible de récompenses.
                En mode exploration, il favorise les déplacements vers des zones inexplorées.
            </p>
            <h3>Méthode mathématique utilisée pour le mouvement de l'agent</h3>
            <p>
                Le mouvement de l'agent est basé sur une méthode mathématique appelée <strong>propagation des valeurs d'état</strong>. Cette méthode consiste à attribuer une valeur à chaque cellule de la grille en fonction de la distance par rapport aux récompenses et aux obstacles.
                La propagation des valeurs d'état est une technique courante en intelligence artificielle pour guider un agent autonome dans un environnement.
            </p>
            <h4>Calcul des valeurs d'état</h4>
            <p>
                Les valeurs d'état pour chaque cellule sont calculées en tenant compte des éléments suivants :
                <ul>
<li><strong>Récompenses :</strong>&nbsp;Les cellules proches des récompenses reçoivent une valeur positive élevée. Cette valeur diminue exponentiellement avec la distance. Par exemple, une cellule à une distance <span class="katex--inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>d</mi></mrow><annotation encoding="application/x-tex">d</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height: 0.69444em; vertical-align: 0em;"></span><span class="mord mathnormal">d</span></span></span></span></span> d’une récompense pourrait recevoir une valeur calculée par la formule <span class="katex--inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>V</mi><mo stretchy="false">(</mo><mi>x</mi><mo separator="true">,</mo><mi>y</mi><mo stretchy="false">)</mo><mo>=</mo><mfrac><msub><mi>V</mi><mrow><mtext>r</mtext><mover accent="true"><mtext>e</mtext><mo>ˊ</mo></mover><mtext>compense</mtext></mrow></msub><mrow><mn>1</mn><mo>+</mo><mi>d</mi><mo stretchy="false">(</mo><mi>x</mi><mo separator="true">,</mo><mi>y</mi><mo stretchy="false">)</mo></mrow></mfrac></mrow><annotation encoding="application/x-tex">V(x, y) = \frac{V_{\text{récompense}}}{1 + d(x, y)}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height: 1em; vertical-align: -0.25em;"></span><span class="mord mathnormal" style="margin-right: 0.22222em;">V</span><span class="mopen">(</span><span class="mord mathnormal">x</span><span class="mpunct">,</span><span class="mspace" style="margin-right: 0.166667em;"></span><span class="mord mathnormal" style="margin-right: 0.03588em;">y</span><span class="mclose">)</span><span class="mspace" style="margin-right: 0.277778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right: 0.277778em;"></span></span><span class="base"><span class="strut" style="height: 1.51141em; vertical-align: -0.52em;"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.991411em;"><span class="" style="top: -2.655em;"><span class="pstrut" style="height: 3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">1</span><span class="mbin mtight">+</span><span class="mord mathnormal mtight">d</span><span class="mopen mtight">(</span><span class="mord mathnormal mtight">x</span><span class="mpunct mtight">,</span><span class="mord mathnormal mtight" style="margin-right: 0.03588em;">y</span><span class="mclose mtight">)</span></span></span></span><span class="" style="top: -3.23em;"><span class="pstrut" style="height: 3em;"></span><span class="frac-line" style="border-bottom-width: 0.04em;"></span></span><span class="" style="top: -3.51308em;"><span class="pstrut" style="height: 3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right: 0.22222em;">V</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.3448em;"><span class="" style="top: -2.34877em; margin-left: -0.22222em; margin-right: 0.0714286em;"><span class="pstrut" style="height: 2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight"><span class="mord text mtight"><span class="mord mtight">r</span><span class="mord accent mtight"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height: 0.69444em;"><span class="" style="top: -2.69444em;"><span class="pstrut" style="height: 2.69444em;"></span><span class="mord mtight">e</span></span><span class="" style="top: -2.69444em;"><span class="pstrut" style="height: 2.69444em;"></span><span class="accent-body" style="left: -0.25em;"><span class="mord mtight">ˊ</span></span></span></span></span></span></span><span class="mord mtight">compense</span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.290114em;"><span class=""></span></span></span></span></span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.52em;"><span class=""></span></span></span></span></span><span class="mclose nulldelimiter"></span></span></span></span></span></span>, où <span class="katex--inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>V</mi><mrow><mtext>r</mtext><mover accent="true"><mtext>e</mtext><mo>ˊ</mo></mover><mtext>compense</mtext></mrow></msub></mrow><annotation encoding="application/x-tex">V_{\text{récompense}}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height: 0.969438em; vertical-align: -0.286108em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right: 0.22222em;">V</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.336108em;"><span class="" style="top: -2.55em; margin-left: -0.22222em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord text mtight"><span class="mord mtight">r</span><span class="mord accent mtight"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height: 0.69444em;"><span class="" style="top: -2.7em;"><span class="pstrut" style="height: 2.7em;"></span><span class="mord mtight">e</span></span><span class="" style="top: -2.7em;"><span class="pstrut" style="height: 2.7em;"></span><span class="accent-body" style="left: -0.25em;"><span class="mord mtight">ˊ</span></span></span></span></span></span></span><span class="mord mtight">compense</span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.286108em;"><span class=""></span></span></span></span></span></span></span></span></span></span> est la valeur de la récompense et <span class="katex--inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>d</mi><mo stretchy="false">(</mo><mi>x</mi><mo separator="true">,</mo><mi>y</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">d(x, y)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height: 1em; vertical-align: -0.25em;"></span><span class="mord mathnormal">d</span><span class="mopen">(</span><span class="mord mathnormal">x</span><span class="mpunct">,</span><span class="mspace" style="margin-right: 0.166667em;"></span><span class="mord mathnormal" style="margin-right: 0.03588em;">y</span><span class="mclose">)</span></span></span></span></span> est la distance.</li>
                    <li><strong>Obstacles :</strong> Les cellules proches des obstacles reçoivent une valeur négative, également diminuée exponentiellement avec la distance. Cela permet à l'agent d'éviter ces cellules.</li>
                </ul>
            </p>
            <h4>Choix de la direction</h4>
            <p>
                L'agent choisit sa direction en évaluant les valeurs d'état des cellules voisines (Nord, Sud, Est, Ouest). Il se déplace vers la cellule ayant la valeur d'état la plus élevée, sauf si un obstacle est détecté, auquel cas il choisit une alternative.
                En mode "survie", l'agent priorise les directions qui augmentent ses chances de trouver une récompense pour récupérer de la santé.
            </p>
            <p>
                En mode "exploration", l'agent peut choisir de se déplacer vers des zones inexplorées, même si leur valeur d'état est faible, pour maximiser la découverte de l'environnement.
            </p>
            <h4>Propagation des valeurs</h4>
            <p>
                La propagation des valeurs d'état est effectuée à chaque mouvement de l'agent pour prendre en compte les nouvelles informations découvertes sur la grille. Ce recalcul constant permet à l'agent d'adapter ses mouvements de manière optimale en fonction de l'évolution de son environnement.
            </p>
            <p>
                Cette méthode mathématique assure que l'agent se déplace de manière intelligente, en minimisant les risques (éviter les obstacles) et en maximisant les gains (collecter des récompenses), tout en explorant efficacement le territoire.
            </p>
            <h3>Affichage en temps réel</h3>
            <p>
                Le script met à jour l'interface utilisateur en temps réel pour afficher la santé de l'agent, sa dernière direction choisie, et la valeur de l'état de la cellule où il se trouve. Ces informations sont cruciales pour suivre la progression de l'agent dans l'environnement.
            </p>
            <h3>Événements et interactions</h3>
            <p>
                Un événement de survol de la souris sur le canvas permet de voir la valeur d'état des cellules en temps réel. Cela est utile pour comprendre pourquoi l'agent choisit certaines directions en fonction de l'attractivité des cellules.
            </p>
            <p>
                En conclusion, ce script offre une simulation dynamique et interactive d'un agent autonome naviguant dans un environnement complexe, avec des comportements d'exploration et de survie basés sur les principes de la propagation de valeurs d'état et de la mémoire des positions visitées.
            </p>
        </div>
    </div>
    <div class="info">
        <p style="margin:0;padding:0;">Points de vie de l'agent : <span id="health"></span></p>
        <p style="margin:0;padding:0;">Dernier choix de direction : <span id="direction"></span></p>
        <p style="margin:0;padding:0;">Valeur de l'état : <span id="stateValue">0</span></p>
    </div>
    <script src="agent.js"></script>
</body>
</html>
