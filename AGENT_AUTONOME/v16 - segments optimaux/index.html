<!DOCTYPE html>
<html lang="fr">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Exploration optimale avec récompenses et obstacles</title>
    <style>
        canvas {
            border: 1px solid black;
            background-color: #f0f0f0;
        }
        .info {
            margin-top: 10px;
        }
        .explanation {
            margin-left: 20px;
			max-height: 600px;
			overflow: auto;
            font-size: 14px;
            line-height: 1.5;
        }
        .container {
            display: flex;
            align-items: flex-start;
        }
    </style>
</head>
<body>
    <div class="container">
        <canvas id="territory" width="600" height="600"></canvas>
		<div class="explanation">
			<h1>Explication détaillée de l'Agent Autonome</h1>

			<h2>Vue d'ensemble</h2>
			<p>Ce code implémente une simulation d'un agent autonome explorant un environnement en grille 2D. L'environnement contient des obstacles et des récompenses, et l'agent doit naviguer efficacement pour collecter des récompenses tout en évitant les obstacles et en gérant son niveau d'énergie. L'agent utilise une approche heuristique basée sur la propagation de valeurs pour prendre ses décisions de mouvement.</p>

			<h2>Composants clés</h2>

			<h3>1. Environnement</h3>
			<p>L'environnement est représenté par une grille 2D, où chaque cellule peut être :</p>
			<ul>
				<li>Vide</li>
				<li>Un obstacle</li>
				<li>Une récompense</li>
			</ul>
			<p>La grille est implémentée comme un canvas HTML, où chaque cellule a une taille définie par <code>config.stepSize</code>.</p>

			<h3>2. Agent</h3>
			<p>L'agent est l'entité principale qui explore l'environnement. Il a les propriétés suivantes :</p>
			<ul>
				<li>Position (x, y)</li>
				<li>Santé (diminue à chaque mouvement)</li>
				<li>Mode (exploration ou survie)</li>
				<li>Couleur (change selon le mode)</li>
			</ul>

			<h3>3. Système de valeurs d'état</h3>
			<p>Un système de valeurs d'état est utilisé pour guider le comportement de l'agent. Chaque cellule de la grille a une valeur d'état associée.</p>

			<h2>Algorithmes et mathématiques</h2>

			<h3>1. Génération de l'environnement</h3>
			<p>La fonction <code>generateObstaclesAndRewards()</code> utilise un générateur de nombres aléatoires pour placer les obstacles et les récompenses. Elle utilise un ensemble <code>occupiedCells</code> pour s'assurer qu'une cellule n'est pas à la fois un obstacle et une récompense.</p>

			<h3>2. Propagation des valeurs</h3>
			<p>La fonction <code>propagateValues()</code> est au cœur du système de prise de décision de l'agent. Elle propage les valeurs des récompenses découvertes à travers la grille. L'algorithme fonctionne comme suit :</p>
			<ol>
				<li>Initialise toutes les valeurs d'état à 0.</li>
				<li>Pour chaque récompense découverte :
					<ol type="a">
						<li>Définit la valeur de la cellule de récompense à l'infini.</li>
						<li>Pour chaque cellule de la grille, calcule une valeur basée sur la distance à la récompense :
							<code>value = γ^(distance / stepSize) * rewardValue</code>
							où γ (gamma) est un facteur de décroissance (0 < γ < 1).</li>
						<li>Met à jour la valeur d'état de la cellule si la nouvelle valeur est supérieure à la valeur existante.</li>
					</ol>
				</li>
				<li>Définit la valeur des cellules d'obstacles à -1.</li>
			</ol>

			<h3>3. Prise de décision de l'agent</h3>
			<p>L'agent utilise deux stratégies principales pour décider de son mouvement :</p>
			<ol>
				<li><strong>Mode survie</strong> (<code>findBestDirection</code>) :
					<ul>
						<li>Si des récompenses ont été découvertes, l'agent tente de construire un chemin optimal vers la récompense la plus proche.</li>
						<li>Si aucun chemin n'est trouvé ou si aucune récompense n'a été découverte, l'agent choisit la direction avec la valeur d'état la plus élevée, en tenant compte du nombre de visites précédentes pour éviter de rester bloqué.</li>
					</ul>
				</li>
				<li><strong>Mode exploration</strong> (<code>findExplorationDirection</code>) :
					<ul>
						<li>L'agent privilégie les cellules inexplorées.</li>
						<li>S'il n'y en a pas, il revient à la stratégie du mode survie.</li>
					</ul>
				</li>
			</ol>

			<h3>4. Découverte de l'environnement</h3>
			<p>La fonction <code>discoverSurroundings()</code> simule la capacité de l'agent à "voir" les cellules adjacentes, mettant à jour ses connaissances sur les obstacles et les récompenses à proximité.</p>

			<h2>Concepts mathématiques clés</h2>
			<ol>
				<li><strong>Décroissance exponentielle</strong> : La valeur propagée des récompenses décroît exponentiellement avec la distance, suivant la formule γ^d, où d est la distance.</li>
				<li><strong>Maximisation des valeurs</strong> : Lors de la propagation, on utilise <code>Math.max()</code> pour conserver uniquement l'influence de la récompense la plus forte pour chaque cellule.</li>
				<li><strong>Heuristique de mouvement</strong> : L'agent utilise une combinaison de valeurs d'état et de comptage des visites pour décider de son mouvement, créant un équilibre entre l'exploitation (collecter des récompenses connues) et l'exploration (découvrir de nouvelles zones).</li>
				<li><strong>Planification de chemin</strong> : En mode survie, l'agent tente de construire un chemin optimal vers la récompense la plus proche, utilisant une recherche en largeur simplifiée.</li>
			</ol>

			<h2>Améliorations potentielles</h2>
			<ol>
				<li>Optimiser l'algorithme de planification de chemin pour une navigation plus efficace vers les récompenses connues, potentiellement en implémentant A*.</li>
				<li>Ajouter un mécanisme d'apprentissage par renforcement pour que l'agent améliore sa stratégie au fil du temps.</li>
				<li>Introduire des récompenses dynamiques ou des obstacles mobiles pour un environnement plus complexe.</li>
				<li>Optimiser la fonction de propagation des valeurs pour de meilleures performances sur de grandes grilles.</li>
				<li>Améliorer la gestion de l'énergie de l'agent, en tenant compte de la distance aux points de recharge lors de la prise de décision.</li>
			</ol>

			<p>Cette simulation offre une base solide pour explorer des concepts d'intelligence artificielle et de prise de décision autonome dans un environnement contrôlé, tout en introduisant des défis réalistes comme la gestion des ressources (énergie de l'agent) et la navigation dans des environnements complexes.</p>
		</div>
    </div>
    <div class="info">
        <p style="margin:0;padding:0;">Points de vie de l'agent : <span id="health"></span></p>
        <p style="margin:0;padding:0;">Dernier choix de direction : <span id="direction"></span></p>
        <p style="margin:0;padding:0;">Valeur de l'état : <span id="stateValue">0</span></p>
		<button id="playPauseBtn">Pause</button>
    </div>
    <script src="agent.js"></script>
</body>
</html>
